{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Subjectivity of Skin Tone: Evaluating Patient and Annotator Agreement Across Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Load Annotator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotator data from Excel (all sheets)\n",
    "def load_annotator_data(file_path):\n",
    "    all_sheets_data = pd.read_excel(file_path, sheet_name=None)\n",
    "    consolidated_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name, sheet_data in all_sheets_data.items():\n",
    "        annotator = sheet_name.split(' ')[1].strip()\n",
    "        scale = sheet_name.split(' ')[2].strip()\n",
    "        # Rename for homogenaity\n",
    "        if scale == 'Fitz':\n",
    "            scale = 'Fitzpatrick'\n",
    "\n",
    "        sheet_data['Annotator'] = annotator\n",
    "        sheet_data['Scale'] = scale\n",
    "        # Rename the 'Confidence Score' column to include the scale name\n",
    "        sheet_data.rename(columns={'Confidence Score': f'{scale} Confidence'}, inplace=True)\n",
    "        consolidated_df = pd.concat([consolidated_df, sheet_data], ignore_index=True)\n",
    "    \n",
    "    return consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_file_path = 'replace_with_your_data_path_here'\n",
    "\n",
    "annotator_data = load_annotator_data(your_file_path)\n",
    "\n",
    "print(annotator_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_annotator_data(df):\n",
    "    # Extract unique image name (last part after \"/\") and parse Subject ID and Location\n",
    "    df['Image_ID'] = df['Image Name'].apply(lambda x: x.split('/')[-1])\n",
    "    df['Subject_ID'] = df['Image_ID'].apply(lambda x: re.search(r'\\d+', x.split('_')[0]).group() if re.search(r'\\d+', x.split('_')[0]) else None)\n",
    "    \n",
    "    def extract_location(image_id):\n",
    "        if 'forehead' in image_id:\n",
    "            return 'forehead'\n",
    "        elif 'right_cheek' in image_id:\n",
    "            return 'right cheek'\n",
    "        elif 'left_cheek' in image_id:\n",
    "            return 'left cheek'\n",
    "        return 'unknown'  # If location is unspecified, label it as 'unknown'\n",
    "    \n",
    "    df['Location'] = df['Image_ID'].apply(extract_location)\n",
    "    \n",
    "    # Filter out rows where 'Subject_ID' contains scale names (like 'fenty', 'monk', 'fitz') -- the scale images need to be excluded\n",
    "    df = df[~df['Subject_ID'].isin(['fenty', 'monk', 'fitz'])]\n",
    "\n",
    "    # Initialize an empty list to collect the consolidated rows\n",
    "    consolidated_data = []\n",
    "\n",
    "    # Group by unique image and Subject_ID to consolidate entries\n",
    "    for (image_id, subject_id, location), group in df.groupby(['Image_ID', 'Subject_ID', 'Location']):\n",
    "        # Create a dictionary to store consolidated data for the image\n",
    "        consolidated_entry = {\n",
    "            'Image_ID': image_id,\n",
    "            'Subject_ID': subject_id,\n",
    "            'Location': location,\n",
    "            'Image Name': group['Image Name'].iloc[0],  # Use the first instance\n",
    "            'Timestamp': group['Timestamp'].iloc[0]      # Use the first timestamp instance\n",
    "        }\n",
    "        \n",
    "        # Loop over each annotator's entry in the group\n",
    "        for _, row in group.iterrows():\n",
    "            scale = row['Scale']\n",
    "            annotator = row['Annotator']\n",
    "            \n",
    "            # Add scale-specific scores and confidence with annotator designation\n",
    "            score_column = f'{scale} Score'\n",
    "            confidence_column = f'{scale} Confidence'\n",
    "\n",
    "            # Add scale-specific scores and confidence with annotator designation\n",
    "            consolidated_entry[f'{scale}_Score_Annotator{annotator}'] = row[score_column]\n",
    "            consolidated_entry[f'{scale}_Confidence_Annotator{annotator}'] = row[confidence_column]\n",
    "        \n",
    "        # Append the consolidated entry to the list\n",
    "        consolidated_data.append(consolidated_entry)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    consolidated_df = pd.DataFrame(consolidated_data)\n",
    "\n",
    "    # Return the consolidated DataFrame with unique Image_IDs and Annotator data as separate columns\n",
    "    return consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_annotator_data = consolidate_annotator_data(annotator_data)\n",
    "print(consolidated_annotator_data.head())\n",
    "# print the size of the df\n",
    "print(consolidated_annotator_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Load Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic data and standardize columns\n",
    "def load_demographic_data(file_path):\n",
    "    demographic_data = pd.read_excel(file_path)\n",
    "    demographic_data.columns = demographic_data.columns.str.strip()\n",
    "    # Replace Subject Number with Subject ID\n",
    "    demographic_data.rename(columns={'Subject Number': 'Subject_ID'}, inplace=True)\n",
    "    return demographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data = load_demographic_data('data/Patient_Demographics.xlsx')\n",
    "\n",
    "print(demographic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Set Global Vars \n",
    "- Color Palette Variables: These hex values were extracted using this link (https://imagecolorpicker.com/) and the images used were the offical Monk Scale on Google's website (https://skintone.google/) and the fitzpatrick scale from this paper (https://pmc.ncbi.nlm.nih.gov/articles/PMC10566767/#:~:text=&text=The%20Fitzpatrick%20scale%20classifies%20skin,6%20as%20the%20darkest%20shade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitzpatrick_palette = {\n",
    "    \"1.0\": \"#c6b49d\", \n",
    "    \"2.0\": \"#bea48b\",  \n",
    "    \"3.0\": \"#af9479\", \n",
    "    \"4.0\": \"#a58367\",  \n",
    "    \"5.0\": \"#896952\",  \n",
    "    \"6.0\": \"#675043\" \n",
    "}\n",
    "\n",
    "monk_palette = {\n",
    "    \"1.0\": \"#f5ede4\",\n",
    "    \"2.0\": \"#f3e7db\",\n",
    "    \"3.0\": \"#f7eacf\",\n",
    "    \"4.0\": \"#eadabb\",\n",
    "    \"5.0\": \"#d7bd96\",\n",
    "    \"6.0\": \"#a07e56\",\n",
    "    \"7.0\": \"#825c43\", \n",
    "    \"8.0\": \"#604134\",\n",
    "    \"9.0\": \"#3a312a\",\n",
    "    \"10.0\": \"#2a2520\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing & Merging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Helper Functions\n",
    "- Roman Numeral Converter \n",
    "- Categorizing Races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roman_to_int(roman):\n",
    "    if pd.isna(roman):\n",
    "        return None\n",
    "    elif isinstance(roman, float):\n",
    "        return roman\n",
    "    roman_map = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5, 'VI': 6}\n",
    "    return roman_map.get(roman.upper().strip(), None)\n",
    "\n",
    "def categorize_race_ethnicity(race_ethnicity):\n",
    "    if pd.isna(race_ethnicity):\n",
    "        return 'Unknown'\n",
    "    race_ethnicity = race_ethnicity.lower().strip()\n",
    "    categories = {\n",
    "        'hispanic/latino': 'Hispanic/Latino',\n",
    "        'white/white(not hispanic or latino)': 'White',\n",
    "        'african american/black': 'African American/Black',\n",
    "        'native american': 'Native American/Alaska Native',\n",
    "        'native hawaiian/pacific islander': 'Native Hawaiian/Pacific Islander',\n",
    "        'asian': 'Asian',\n",
    "        'declined': 'Unknown',\n",
    "        'other': 'Other'\n",
    "    }\n",
    "    for key, val in categories.items():\n",
    "        if key in race_ethnicity:\n",
    "            return val\n",
    "    return 'Multiracial' if any(x in race_ethnicity for x in ['\\\\n', '&', 'and']) else 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Merge Annotator and Patient Demographics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(annotator_data, demographic_data):\n",
    "    # Group Race and Ethnicity \n",
    "    demographic_data['Race (Grouped)'] = demographic_data['Race/Ethnicity'].apply(categorize_race_ethnicity)\n",
    "    demographic_data['Fitzpatrick Score'] = demographic_data['Fitzpatrick Score'].apply(roman_to_int)\n",
    "    \n",
    "    # Strip whitespace in all string columns for both DataFrames\n",
    "    annotator_data = annotator_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    demographic_data = demographic_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    # make sure both subject IDs are strs\n",
    "    annotator_data['Subject_ID'] = annotator_data['Subject_ID'].astype(str)\n",
    "    demographic_data['Subject_ID'] = demographic_data['Subject_ID'].astype(str)\n",
    "    \n",
    "    # Merg the data via the Subject ID \n",
    "    merged_df = annotator_data.merge(demographic_data, on='Subject_ID', how='left')\n",
    "\n",
    "    # drop any rows missing a subjective score \n",
    "    merged_df = merged_df.dropna(subset=['Fitzpatrick Score'])\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = merge_data(consolidated_annotator_data, demographic_data)\n",
    "print(master_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Patient Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_demographics(merged_df):\n",
    "    # Keep only one entry per unique patient (based on Subject_ID)\n",
    "    unique_patients_df = merged_df.drop_duplicates(subset=['Subject_ID'])\n",
    "\n",
    "    #print the number of unique patients\n",
    "    print(f'The number of unique patients is {unique_patients_df.shape[0]}')\n",
    "\n",
    "    # Plot configurations as previously set up, but now using `unique_patients_df`\n",
    "    plot_configs = [\n",
    "        {\"col\": \"Age\", \"kind\": \"hist\", \"title\": \"Age Distribution\", \"x_label\": \"Age\", \"kde\": True, \"bins\": 10, \"color\": \"blue\"},\n",
    "        {\"col\": \"Sex\", \"kind\": \"count\", \"title\": \"Sex Distribution\", \"x_label\": \"Sex\", \"palette\": \"Set2\"},\n",
    "        {\"col\": \"Race (Grouped)\", \"kind\": \"count\", \"title\": \"Race Distribution\", \"y_label\": \"Race\", \"palette\": \"Set3\"},\n",
    "        {\"col\": \"Fitzpatrick Score\", \"kind\": \"hist\", \"title\": \"Fitzpatrick Score Distribution\", \"x_label\": \"Fitzpatrick Score\", \"kde\": True, \"bins\": 6, \"color\": \"grey\"},\n",
    "        {\"col\": \"Monk Score\", \"kind\": \"hist\", \"title\": \"Monk Score Distribution\", \"x_label\": \"Monk Score\", \"kde\": True, \"bins\": 10, \"color\": \"grey\"},\n",
    "        {\"col\": \"NATIVE LANGUAGE\", \"kind\": \"count\", \"title\": \"Native Language Distribution\", \"y_label\": \"Native Language\", \"palette\": \"Set1\"},\n",
    "        {\"col\": \"Substance Use\", \"kind\": \"count\", \"title\": \"Substance Use Distribution\", \"y_label\": \"Substance Use\", \"palette\": \"Set2\"}\n",
    "    ]\n",
    "\n",
    "    for config in plot_configs:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if config[\"kind\"] == \"hist\":\n",
    "            sns.histplot(unique_patients_df[config[\"col\"]], kde=config.get(\"kde\", False), bins=config.get(\"bins\", 10), color=config.get(\"color\"))\n",
    "        elif config[\"kind\"] == \"count\":\n",
    "            sns.countplot(y=config[\"col\"], data=unique_patients_df, palette=config.get(\"palette\"))\n",
    "        \n",
    "        plt.title(config[\"title\"])\n",
    "        plt.xlabel(config.get(\"x_label\", \"Count\"))\n",
    "        plt.ylabel(config.get(\"y_label\", \"Frequency\"))\n",
    "        plt.show()\n",
    "    \n",
    "        # Calculate the percentage of the cols unique values\n",
    "        data = unique_patients_df[config[\"col\"]]\n",
    "        data = data.value_counts(normalize=True) * 100\n",
    "        print(data)\n",
    "\n",
    "        if config[\"col\"] == 'Age': \n",
    "            # find the median age \n",
    "            median_age = unique_patients_df['Age'].median()\n",
    "            print(f'The median age is {median_age}')\n",
    "            # Find the IQR of the age\n",
    "            Q1 = unique_patients_df['Age'].quantile(0.25)\n",
    "            Q3 = unique_patients_df['Age'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            print(f'The IQR of the age is {IQR}')\n",
    "\n",
    "plot_demographics(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics TABLE \n",
    "import pandas as pd\n",
    "def generate_demographics_table(master_df):\n",
    "    # Keep only unique patients\n",
    "    unique_patients_df = master_df.drop_duplicates(subset=['Subject_ID']).copy()\n",
    "    total_patients = unique_patients_df.shape[0]\n",
    "\n",
    "    print(unique_patients_df.head())    \n",
    "\n",
    "    # Helper function to group small categories into 'Other'\n",
    "    def group_small_categories(series, threshold=0.05):\n",
    "        value_counts = series.value_counts(normalize=True)\n",
    "        small_categories = value_counts[value_counts < threshold].index\n",
    "        return series.replace(small_categories, 'Other')\n",
    "\n",
    "    # Apply CMS cell suppression for each column\n",
    "    unique_patients_df['Sex'] = group_small_categories(unique_patients_df['Sex'])\n",
    "    unique_patients_df['Race (Grouped)'] = group_small_categories(unique_patients_df['Race (Grouped)'])\n",
    "    unique_patients_df['NATIVE LANGUAGE'] = group_small_categories(unique_patients_df['NATIVE LANGUAGE'])\n",
    "    unique_patients_df['Substance Use'] = group_small_categories(unique_patients_df['Substance Use'])\n",
    "\n",
    "    # Calculate demographic distributions\n",
    "    demographics_table = {\n",
    "        'Total Patients': total_patients,\n",
    "        'Median Age': unique_patients_df['Age'].median(),\n",
    "        'Age IQR': f\"{unique_patients_df['Age'].quantile(0.25)} - {unique_patients_df['Age'].quantile(0.75)}\",\n",
    "        'Sex Distribution': unique_patients_df['Sex'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "        'Race Distribution': unique_patients_df['Race (Grouped)'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "        'Native Language Distribution': unique_patients_df['NATIVE LANGUAGE'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "        'Substance Use Distribution': unique_patients_df['Substance Use'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "        'Fitzpatrick Score Distribution': unique_patients_df['Fitzpatrick Score'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "        'Monk Score Distribution': unique_patients_df['Monk Score'].value_counts(normalize=True).map(lambda x: f\"{x * 100:.2f}%\").to_dict(),\n",
    "    }\n",
    "\n",
    "    # Flatten the nested dictionary for DataFrame conversion\n",
    "    rows = []\n",
    "    for key, value in demographics_table.items():\n",
    "        if isinstance(value, dict):  # For distribution columns\n",
    "            for sub_key, sub_value in value.items():\n",
    "                rows.append({'Variable': key, 'Category': sub_key, 'Value': sub_value})\n",
    "        else:\n",
    "            rows.append({'Variable': key, 'Category': None, 'Value': value})\n",
    "\n",
    "    formatted_table = pd.DataFrame(rows)\n",
    "    return formatted_table\n",
    "\n",
    "# Example usage\n",
    "demographics_table = generate_demographics_table(master_df)\n",
    "\n",
    "# Display the table in Jupyter Notebook\n",
    "from IPython.display import display\n",
    "display(demographics_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Comparative Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fitzpatrick Score by Race/Ethnicity\n",
    "# Keep only unique patients\n",
    "unique_patients_df = master_df.drop_duplicates(subset=['Subject_ID']).copy()\n",
    "\n",
    "# Helper function to group small categories into 'Other'\n",
    "def group_small_categories(series, threshold=0.05):\n",
    "    value_counts = series.value_counts(normalize=True)\n",
    "    small_categories = value_counts[value_counts < threshold].index\n",
    "    return series.replace(small_categories, 'Other')\n",
    "\n",
    "# Apply CMS cell suppression for each column\n",
    "unique_patients_df['Race (Grouped)'] = group_small_categories(unique_patients_df['Race (Grouped)'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Fitzpatrick Score\", y=\"Race (Grouped)\", data=unique_patients_df, palette=\"deep\")\n",
    "plt.title(\"Fitzpatrick Score by Self-Reported Race\")\n",
    "plt.xlabel(\"Fitzpatrick Score\")\n",
    "plt.ylabel(\"Self-Reported Race\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Monk Score by Race/Ethnicity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Monk Score\", y=\"Race (Grouped)\", data=unique_patients_df, palette=\"deep\")\n",
    "plt.title(\"Monk Score by Self-Reported Race\")\n",
    "plt.xlabel(\"Monk Score\")\n",
    "plt.ylabel(\"Self-Reported Race\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stacked_skin_tone_histogram(df, score_col, race_col='Race (Grouped)', \n",
    "                                     bins=10, palette='Set2', \n",
    "                                     title=None, x_label=None, y_label='Count'):\n",
    "    \"\"\"\n",
    "    Plots a stacked histogram of the skin tone scores (e.g., Fitzpatrick or Monk) \n",
    "    with Race/Ethnicity groups stacked within each bin.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing at least the skin tone column and the race/ethnicity column.\n",
    "    score_col : str\n",
    "        The name of the column containing the skin tone scores (e.g. 'Fitzpatrick Score' or 'Monk Score').\n",
    "    race_col : str\n",
    "        The name of the column containing race/ethnicity group labels.\n",
    "    bins : int\n",
    "        Number of bins to use in the histogram.\n",
    "    palette : str or list\n",
    "        Seaborn color palette or list of colors.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    x_label : str\n",
    "        X-axis label.\n",
    "    y_label : str\n",
    "        Y-axis label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default labels if not provided\n",
    "    if title is None:\n",
    "        title = f\"Distribution of {score_col} by {race_col}\"\n",
    "    if x_label is None:\n",
    "        x_label = score_col\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the stacked histogram\n",
    "    ax = sns.histplot(\n",
    "        data=df,\n",
    "        x=score_col,\n",
    "        hue=race_col,\n",
    "        multiple='stack',\n",
    "        bins=bins,\n",
    "        palette=palette,\n",
    "        edgecolor='black',\n",
    "        legend='full'\n",
    "    )\n",
    "\n",
    "    # -- Force legend creation logic --\n",
    "    # Try to grab the automatically-created legend\n",
    "    auto_legend = ax.get_legend()\n",
    "\n",
    "    if auto_legend is None:\n",
    "        # If no legend was created automatically, create one manually\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # Sometimes the first label is just the name of the hue variable; remove if so\n",
    "        if labels and labels[0] == race_col:\n",
    "            handles, labels = handles[1:], labels[1:]\n",
    "        # Only create a legend if we actually have more than 0 categories\n",
    "        if labels:\n",
    "            plt.legend(handles, labels, title=race_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        # If a legend did get created, rename the title, move it, etc.\n",
    "        auto_legend.set_title(race_col)\n",
    "        auto_legend.set_bbox_to_anchor((1.05, 1))\n",
    "        auto_legend.set_loc('upper left')\n",
    "\n",
    "    # Aesthetics\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# NEW stacked histogram usage\n",
    "plot_stacked_skin_tone_histogram(\n",
    "    df=unique_patients_df, \n",
    "    score_col='Fitzpatrick Score', \n",
    "    race_col='Race (Grouped)', \n",
    "    bins=6, \n",
    "    palette='deep', \n",
    "    title='Stacked Histogram of Fitzpatrick Score by Self-Reported Race', \n",
    "    x_label='Fitzpatrick Score'\n",
    ")\n",
    "\n",
    "# If you also want a similar stacked histogram for Monk Score:\n",
    "plot_stacked_skin_tone_histogram(\n",
    "    df=unique_patients_df, \n",
    "    score_col='Monk Score', \n",
    "    race_col='Race (Grouped)', \n",
    "    bins=10, \n",
    "    palette='deep', \n",
    "    title='Stacked Histogram of Monk Score by Self-Reported Race', \n",
    "    x_label='Monk Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Confidence Score Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_scores_histogram(merged_df, scales, annotators, num_bins=5, bin_range=(1, 5), palette=\"Set2\"):\n",
    "    # Define colors and labels using Seaborn palette\n",
    "    colors = sns.color_palette(palette, len(annotators))\n",
    "    labels = [f'Annotator {i}' for i in annotators]\n",
    "    bar_width = 0.25\n",
    "    x = np.arange(num_bins)  # Bin positions on the x-axis\n",
    "\n",
    "    # Iterate over each scale\n",
    "    for scale in scales:\n",
    "        # Create a dictionary to hold the histogram counts for each annotator\n",
    "        annotator_histograms = []\n",
    "        \n",
    "        # Calculate histogram counts for each annotator\n",
    "        for annotator in annotators:\n",
    "            confidence_column = f'{scale}_Confidence_Annotator{annotator}'\n",
    "            if confidence_column in merged_df.columns:\n",
    "                # Calculate histogram for the current annotator\n",
    "                annotator_histogram, _ = np.histogram(merged_df[confidence_column].dropna(), bins=num_bins, range=bin_range)\n",
    "                annotator_histograms.append(annotator_histogram)\n",
    "        \n",
    "        # Plot histograms for each annotator, side by side\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, (annotator_histogram, label, color) in enumerate(zip(annotator_histograms, labels, colors)):\n",
    "            plt.bar(x + i * bar_width, annotator_histogram, width=bar_width, label=label, color=color)\n",
    "\n",
    "        # Set plot labels and ticks\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Confidence Scores for {scale} Scale by Annotator')\n",
    "        plt.xticks(x + bar_width, range(1, num_bins + 1))  # Center x-ticks on bins\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Define your scales and annotators\n",
    "scales = ['Fitzpatrick', 'Monk']\n",
    "annotators = [1, 2, 3]\n",
    "\n",
    "# Example usage with Seaborn palette\n",
    "plot_confidence_scores_histogram(master_df, scales, annotators, palette=\"Set2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Missing Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Analysis \n",
    "def missingness_analysis(df):\n",
    "    # Calculate the total and percentage of missing values for each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(df)) * 100\n",
    "    \n",
    "    # Create a summary DataFrame for missing data in the entire dataset\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Values': missing_data,\n",
    "        'Percentage': missing_percentage\n",
    "    }).sort_values(by='Percentage', ascending=False)\n",
    "    \n",
    "    # Filter to include only columns with missing data\n",
    "    missing_summary = missing_summary[missing_summary['Missing Values'] > 0]\n",
    "\n",
    "    print(\"Missingness Analysis Summary for All Columns:\")\n",
    "    print(missing_summary)\n",
    "    \n",
    "    # Detailed analysis for critical columns 'Age' and 'Sex'\n",
    "    missing_age = df[df['Age'].isnull()]\n",
    "    missing_sex = df[df['Sex'].isnull()]\n",
    "    missing_fitz = df[df['Fitzpatrick Score'].isnull()]\n",
    "    missing_monk = df[df['Monk Score'].isnull()]\n",
    "    \n",
    "    if not missing_age.empty:\n",
    "        print(\"\\nDetailed Missingness for 'Age':\")\n",
    "        print(\"Number of missing 'Age' values:\", len(missing_age))\n",
    "        print(\"Missing 'Age' for Subject_IDs:\")\n",
    "        print(missing_age['Subject_ID'].unique())\n",
    "    else:\n",
    "        print(\"\\nNo missing values found for 'Age'.\")\n",
    "    \n",
    "    if not missing_sex.empty:\n",
    "        print(\"\\nDetailed Missingness for 'Sex':\")\n",
    "        print(\"Number of missing 'Sex' values:\", len(missing_sex))\n",
    "        print(\"Missing 'Sex' for Subject_IDs:\")\n",
    "        print(missing_sex['Subject_ID'].unique())\n",
    "    else:\n",
    "        print(\"\\nNo missing values found for 'Sex'.\")\n",
    "\n",
    "    if not missing_fitz.empty:\n",
    "        print(\"\\nDetailed Missingness for 'Fitzpatrick Score':\")\n",
    "        print(\"Number of missing 'Fitzpatrick Score' values:\", len(missing_fitz))\n",
    "        print(\"Missing 'Fitzpatrick Score' for Subject_IDs:\")\n",
    "        print(missing_fitz['Subject_ID'].unique())\n",
    "    else:\n",
    "        print(\"\\nNo missing values found for 'Fitzpatrick Score'.\")\n",
    "    \n",
    "    if not missing_monk.empty:\n",
    "        print(\"\\nDetailed Missingness for 'Monk Score':\")\n",
    "        print(\"Number of missing 'Monk Score' values:\", len(missing_monk))\n",
    "        print(\"Missing 'Monk Score' for Subject_IDs:\")\n",
    "        print(missing_monk['Subject_ID'].unique())\n",
    "    else:\n",
    "        print(\"\\nNo missing values found for 'Monk Score'.\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "missing_data_summary = missingness_analysis(master_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis \n",
    "- Intra Anatator\n",
    "- Inter Rater \n",
    "- Logistic Regression \n",
    "- T-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Intra-Annotator Analysis \n",
    "- Cronbach's Alpha with Bootstrapping for Confidence Intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pingouin import cronbach_alpha\n",
    "\n",
    "# Initialize results storage\n",
    "cronbach_alpha_results = []\n",
    "\n",
    "# Define scales and annotators\n",
    "scales = ['Fitzpatrick', 'Monk']\n",
    "annotators = [1, 2, 3]  # Adjust based on the number of annotators\n",
    "\n",
    "# Bootstrapping function for confidence intervals\n",
    "def bootstrap_cronbach_alpha(data, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform bootstrapping to calculate confidence intervals for Cronbach's Alpha.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The dataset for Cronbach's Alpha calculation.\n",
    "        n_bootstrap (int): Number of bootstrap iterations.\n",
    "        alpha (float): Significance level for the confidence interval (default 0.05 for 95% CI).\n",
    "    \n",
    "    Returns:\n",
    "        (float, float): Lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    alphas = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        resample = data.sample(frac=1, replace=True)  # Resample with replacement\n",
    "        alpha_val, _ = cronbach_alpha(resample)\n",
    "        alphas.append(alpha_val)\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    lower = np.percentile(alphas, 100 * (alpha / 2))\n",
    "    upper = np.percentile(alphas, 100 * (1 - alpha / 2))\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "# Loop through each scale and annotator, calculate Cronbach's Alpha, and store results\n",
    "for scale in scales:\n",
    "    for annotator in annotators:\n",
    "        # Select the relevant score column for the current annotator and scale\n",
    "        score_column = f'{scale}_Score_Annotator{annotator}'\n",
    "        \n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if score_column in master_df.columns:\n",
    "            # Filter the DataFrame for the current scale and annotator\n",
    "            scale_data = master_df[['Subject_ID', score_column]].dropna()\n",
    "            \n",
    "            # Group by Subject_ID and aggregate all images under each subject as separate columns\n",
    "            subject_scores = scale_data.groupby('Subject_ID')[score_column].apply(lambda x: list(x)).to_frame()\n",
    "            \n",
    "            # Convert each list of scores into separate columns for each image within the subject\n",
    "            subject_scores_expanded = pd.DataFrame(subject_scores[score_column].tolist(), index=subject_scores.index)\n",
    "            subject_scores_expanded = subject_scores_expanded.dropna(axis=0)  # Drop rows with any missing values\n",
    "\n",
    "            # Cronbach's Alpha calculation across all images (i.e., columns) for the subject\n",
    "            if subject_scores_expanded.shape[1] > 1:  # Ensure at least 2 columns for Cronbach's Alpha\n",
    "                alpha, _ = cronbach_alpha(subject_scores_expanded)\n",
    "                # Perform bootstrapping for confidence intervals\n",
    "                ci_lower, ci_upper = bootstrap_cronbach_alpha(subject_scores_expanded)\n",
    "                \n",
    "                interpretation = ('Excellent' if alpha >= 0.9 else\n",
    "                                  'Acceptable' if alpha >= 0.8 else\n",
    "                                  'Good' if alpha >= 0.7 else 'Poor')\n",
    "                \n",
    "                # Append the result for each annotator and scale\n",
    "                cronbach_alpha_results.append({\n",
    "                    'Annotator': annotator,\n",
    "                    'Scale': scale,\n",
    "                    'Cronbach\\'s Alpha': round(alpha, 2),\n",
    "                    '95% CI': f\"( {round(ci_lower, 2)}, {round(ci_upper, 2)} )\"\n",
    "                    #'Interpretation': f'{interpretation} reliability'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Not enough data variation for Cronbach's Alpha calculation on {scale} - Annotator {annotator}.\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "cronbach_alpha_results_df = pd.DataFrame(cronbach_alpha_results)\n",
    "print(cronbach_alpha_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Table of the Cronbach's Alpha Results without the Interpretation\n",
    "#cronbach_alpha_results_df.drop(columns=['Interpretation'], inplace=True)\n",
    "\n",
    "# Prepare the table data\n",
    "data = cronbach_alpha_results_df.values\n",
    "columns = cronbach_alpha_results_df.columns\n",
    "\n",
    "# Create a Matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Hide the axes\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create a table\n",
    "table = ax.table(\n",
    "    cellText=data,\n",
    "    colLabels=columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\",\n",
    ")\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(columns))))\n",
    "\n",
    "# Make the header row bold and give it a background color\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    if row == 0:  # Header row\n",
    "        cell.set_fontsize(11)\n",
    "        cell.set_text_props(weight=\"bold\")  # Make text bold\n",
    "        cell.set_facecolor(\"#f4f4f4\")  # Light gray background\n",
    "        cell.set_edgecolor(\"black\")  # Black border\n",
    "    elif row % 2 == 0:  # Alternating row colors\n",
    "        cell.set_facecolor(\"#f9f9f9\")  # Slightly lighter background\n",
    "    else:\n",
    "        cell.set_facecolor(\"white\")  # Standard background\n",
    "\n",
    "# Add a title above the table\n",
    "plt.title(\n",
    "    \"Table X. Reliability Assessment Using Cronbach's Alpha\",\n",
    "    fontsize=12,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.subplots_adjust(top=1)\n",
    "\n",
    "# Save as image or PDF\n",
    "plt.savefig(\"styled_cronbach_alpha_table.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator + Location Analysis\n",
    "# Initialize results storage\n",
    "cronbach_alpha_results = []\n",
    "\n",
    "# Define scales and annotators\n",
    "scales = ['Fitzpatrick', 'Monk']\n",
    "annotators = [1, 2, 3]  # Adjust based on the number of annotators\n",
    "locations = ['forehead', 'right cheek', 'left cheek']  # Predefined locations\n",
    "\n",
    "# Bootstrapping function for confidence intervals\n",
    "def bootstrap_cronbach_alpha(data, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform bootstrapping to calculate confidence intervals for Cronbach's Alpha.\n",
    "    \"\"\"\n",
    "    alphas = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        resample = data.sample(frac=1, replace=True)  # Resample with replacement\n",
    "        alpha_val, _ = cronbach_alpha(resample)\n",
    "        alphas.append(alpha_val)\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    lower = np.percentile(alphas, 100 * (alpha / 2))\n",
    "    upper = np.percentile(alphas, 100 * (1 - alpha / 2))\n",
    "    return lower, upper\n",
    "\n",
    "# Loop through each scale, annotator, and location\n",
    "for scale in scales:\n",
    "    for annotator in annotators:\n",
    "        for location in locations:\n",
    "            # Select the relevant score column for the current annotator and scale\n",
    "            score_column = f'{scale}_Score_Annotator{annotator}'\n",
    "            \n",
    "            # Ensure the column exists in the DataFrame\n",
    "            if score_column in master_df.columns:\n",
    "                # Filter the DataFrame for the current scale, annotator, and location\n",
    "                location_data = master_df[master_df['Location'] == location]\n",
    "                scale_data = location_data[['Subject_ID', score_column]].dropna()\n",
    "                \n",
    "                # Group by Subject_ID and aggregate all images under each subject as separate columns\n",
    "                subject_scores = scale_data.groupby('Subject_ID')[score_column].apply(lambda x: list(x)).to_frame()\n",
    "                \n",
    "                # Convert each list of scores into separate columns for each image within the subject\n",
    "                subject_scores_expanded = pd.DataFrame(subject_scores[score_column].tolist(), index=subject_scores.index)\n",
    "                subject_scores_expanded = subject_scores_expanded.dropna(axis=0)  # Drop rows with any missing values\n",
    "\n",
    "                # Cronbach's Alpha calculation across all images (i.e., columns) for the subject\n",
    "                if subject_scores_expanded.shape[1] > 1:  # Ensure at least 2 columns for Cronbach's Alpha\n",
    "                    alpha, _ = cronbach_alpha(subject_scores_expanded)\n",
    "                    # Perform bootstrapping for confidence intervals\n",
    "                    ci_lower, ci_upper = bootstrap_cronbach_alpha(subject_scores_expanded)\n",
    "                    \n",
    "                    interpretation = ('Excellent' if alpha >= 0.9 else\n",
    "                                      'Acceptable' if alpha >= 0.8 else\n",
    "                                      'Good' if alpha >= 0.7 else 'Poor')\n",
    "                    \n",
    "                    # Append the result for each annotator, scale, and location\n",
    "                    cronbach_alpha_results.append({\n",
    "                        'Annotator': annotator,\n",
    "                        'Scale': scale,\n",
    "                        'Location': location,\n",
    "                        'Cronbach\\'s Alpha': round(alpha, 2),\n",
    "                        '95% CI': f\"( {round(ci_lower, 2)}, {round(ci_upper, 2)} )\",\n",
    "                        'Interpretation': f'{interpretation} reliability'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Not enough data variation for Cronbach's Alpha calculation on {scale} - Annotator {annotator} - Location {location}.\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "cronbach_alpha_results_df = pd.DataFrame(cronbach_alpha_results)\n",
    "print(cronbach_alpha_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Annotator', 'Scale', 'Location', 'Cronbach\\'s Alpha', '95% CI']\n",
    "df = cronbach_alpha_results_df\n",
    "df = df.drop(columns=['Interpretation'])\n",
    "\n",
    "# Sort by Location for grouping\n",
    "df.sort_values(by=['Location','Scale'], inplace=True)\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = df.values\n",
    "table_columns = df.columns\n",
    "\n",
    "# Create a Matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Hide the axes\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create a table\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=table_columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\",\n",
    ")\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(table_columns))))\n",
    "\n",
    "# Make the header row bold and give it a background color\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    if row == 0:  # Header row\n",
    "        cell.set_fontsize(11)\n",
    "        cell.set_text_props(weight=\"bold\")  # Make text bold\n",
    "        cell.set_facecolor(\"#f4f4f4\")  # Light gray background\n",
    "        cell.set_edgecolor(\"black\")  # Black border\n",
    "    elif row % 2 == 0:  # Alternating row colors\n",
    "        cell.set_facecolor(\"#f9f9f9\")  # Slightly lighter background\n",
    "    else:\n",
    "        cell.set_facecolor(\"white\")  # Standard background\n",
    "\n",
    "# Add a title above the table\n",
    "plt.title(\n",
    "    \"Table X. Reliability Assessment Using Cronbach's Alpha by Location\",\n",
    "    fontsize=12,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.subplots_adjust(top=1)\n",
    "\n",
    "# Save as image or PDF\n",
    "plt.savefig(\"styled_cronbach_alpha_table_grouped_by_location.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Inter-Annotator Analysis \n",
    "- Consensus Analysis versus Subjective: Box plot with Jitter, Violin, Scatter with Spearman, Bland-Altman\n",
    "- Annotator Agreement: Weighted Cohen's Kappa, Kendall's W, Krippendorff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Consesus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate Mean Score Across Images for Each Annotator for Each Patient\n",
    "def calculate_patient_level_consensus(master_df, scales, annotators):\n",
    "    patient_level_data = []\n",
    "\n",
    "    for scale in scales:\n",
    "        for subject_id in master_df['Subject_ID'].unique():\n",
    "            patient_data = master_df[master_df['Subject_ID'] == subject_id]\n",
    "            consensus_scores = {}\n",
    "\n",
    "            # Calculate the mean score for each annotator across images for this patient\n",
    "            annotator_means = []\n",
    "            for annotator in annotators:\n",
    "                annotator_column = f'{scale}_Score_Annotator{annotator}'\n",
    "                if annotator_column in patient_data.columns:\n",
    "                    annotator_mean = patient_data[annotator_column].mean()\n",
    "                    annotator_means.append(annotator_mean)\n",
    "            \n",
    "            # Calculate the consensus score as the mean across all annotator means\n",
    "            consensus_score = sum(annotator_means) / len(annotator_means)\n",
    "            \n",
    "            # Get the subject's actual score for the scale\n",
    "            subject_score = patient_data[f'{scale} Score'].iloc[0] if f'{scale} Score' in patient_data.columns else None\n",
    "\n",
    "            # Add to patient-level data\n",
    "            patient_level_data.append({\n",
    "                'Subject_ID': subject_id,\n",
    "                'Scale': scale,\n",
    "                'Consensus_Score': consensus_score,\n",
    "                'Subject_Score': subject_score\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    consensus_df = pd.DataFrame(patient_level_data)\n",
    "    return consensus_df\n",
    "\n",
    "# Example usage\n",
    "scales = ['Fitzpatrick', 'Monk']\n",
    "annotators = [1, 2, 3]\n",
    "patient_level_consensus_df = calculate_patient_level_consensus(master_df, scales, annotators)\n",
    "print(patient_level_consensus_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Consensus vs Subjective Plots\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 1. Scatter Plot with Spearman Correlation for Difference\n",
    "def scatter_spearman_consensus(consensus_df, scale):\n",
    "    consensus_df['Difference'] = consensus_df['Consensus_Score'] - consensus_df['Subject_Score']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=consensus_df['Subject_Score'], y=consensus_df['Difference'])\n",
    "    spearman_corr, _ = stats.spearmanr(consensus_df['Subject_Score'], consensus_df['Difference'], nan_policy='omit')\n",
    "    plt.title(f'Spearman Correlation: {scale} Difference (Consensus - Subject) vs Subject Score')\n",
    "    plt.xlabel('Subject Score')\n",
    "    plt.ylabel('Difference (Consensus - Subject)')\n",
    "    plt.text(0.7, 0.1, f'Spearman r={spearman_corr:.2f}', transform=plt.gca().transAxes)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Box Plot with Jitter for Difference\n",
    "def boxplot_jitter_consensus(consensus_df, scale, custom_palette):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=consensus_df['Subject_Score'], y=consensus_df['Difference'], palette=custom_palette)\n",
    "    sns.stripplot(x=consensus_df['Subject_Score'], y=consensus_df['Difference'], color='black', alpha=0.3)\n",
    "    plt.title(f'{scale} Difference (Consensus - Subject) Distribution by Subject Score')\n",
    "    plt.xlabel('Subject Score')\n",
    "    plt.ylabel('Difference (Consensus - Subject)')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Violin Plot for Difference -- with dynamic dashed lines \n",
    "# Function to check if color is too dark\n",
    "def rgb2gray(r, g, b):\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "def violin_plot_consensus(consensus_df, scale, custom_palette):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Extract hex values from the palette\n",
    "    palette_colors = list(custom_palette.values())\n",
    "    ax = sns.violinplot(x=consensus_df['Subject_Score'], y=consensus_df['Difference'], palette=custom_palette, inner=\"quartile\")\n",
    "    # Determine darkness of the palette colors (convert hex to RGB first)\n",
    "    thresh = 0.6  # Threshold for darkness\n",
    "    dark = [ rgb2gray(*mpl.colors.to_rgb(c)) < thresh for c in palette_colors]\n",
    "        # Count the number of points for each category\n",
    "    counts = consensus_df['Subject_Score'].value_counts().sort_index()\n",
    "    # Track the index of the ax.lines to handle different line counts per violin\n",
    "    line_index = 0\n",
    "    for d, count in zip(dark, counts):\n",
    "        # Each violin should ideally have three lines, but check if there are fewer\n",
    "        if count > 1:\n",
    "            # Only proceed if there are exactly three lines for this violin\n",
    "            violin_lines = ax.lines[line_index:line_index + 3]\n",
    "            line_index += 3  # Move to the next set of lines for the next violin\n",
    "            \n",
    "            # Apply color change if the palette color is dark\n",
    "            if d:\n",
    "                for line in violin_lines:\n",
    "                    line.set_color('white')\n",
    "        else:\n",
    "            # Skip to the next violin without changing line color if only one point\n",
    "            line_index += 1  # Only one line is present for single data points\n",
    "    plt.title(f'Differences Between Annotator Consensus and Patient-Reported {scale} Scores Across Subject Scores')\n",
    "    plt.xlabel('Subject Score')\n",
    "    plt.ylabel('Difference (Consensus - Subject)')\n",
    "    plt.show()\n",
    "\n",
    "# 4. Bland-Altman Plot for Difference\n",
    "def bland_altman_plot_consensus(consensus_df, scale):\n",
    "    consensus = consensus_df['Consensus_Score']\n",
    "    subject_score = consensus_df['Subject_Score']\n",
    "    mean_score = (consensus + subject_score) / 2\n",
    "    diff_score = consensus - subject_score\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(mean_score, diff_score, alpha=0.5)\n",
    "    plt.axhline(diff_score.mean(), color='red', linestyle='--')\n",
    "    plt.axhline(diff_score.mean() + 1.96 * diff_score.std(), color='blue', linestyle='--')\n",
    "    plt.axhline(diff_score.mean() - 1.96 * diff_score.std(), color='blue', linestyle='--')\n",
    "    plt.title(f'Bland-Altman Plot: {scale} Consensus vs Subject Score')\n",
    "    plt.xlabel('Mean Score')\n",
    "    plt.ylabel('Difference (Consensus - Subject)')\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "def bland_altman_plot_consensus_2(consensus_df, scale):\n",
    "    consensus = consensus_df['Consensus_Score']\n",
    "    subject_score = consensus_df['Subject_Score']\n",
    "    mean_score = (consensus + subject_score) / 2\n",
    "    diff_score = consensus - subject_score\n",
    "    \n",
    "    # Compute Spearman's correlation between the mean and the difference\n",
    "    spearman_corr, _ = stats.spearmanr(mean_score, diff_score, nan_policy='omit')\n",
    "\n",
    "    # Create the Bland-Altman plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=mean_score, y=diff_score, alpha=0.5)\n",
    "\n",
    "    # Calculate mean difference and limits of agreement\n",
    "    mean_diff = diff_score.mean()\n",
    "    std_diff = diff_score.std()\n",
    "    upper_limit = mean_diff + 1.96 * std_diff\n",
    "    lower_limit = mean_diff - 1.96 * std_diff\n",
    "\n",
    "    # Add horizontal lines for the mean and limits of agreement\n",
    "    plt.axhline(mean_diff, color='red', linestyle='--', label=f'Mean Diff: {mean_diff:.2f}')\n",
    "    plt.axhline(upper_limit, color='blue', linestyle='--', label=f'Upper Limit: {upper_limit:.2f}')\n",
    "    plt.axhline(lower_limit, color='blue', linestyle='--', label=f'Lower Limit: {lower_limit:.2f}')\n",
    "\n",
    "    # Fit and add a regression line (Loess alternative could be used for nonlinear trends)\n",
    "    sns.regplot(x=mean_score, y=diff_score, scatter=False, color='black',\n",
    "                line_kws={\"linestyle\": \"dashed\"}, label='Regression Line')\n",
    "\n",
    "    # Add legend including Spearman correlation coefficient\n",
    "    plt.legend(title=f'Spearman r: {spearman_corr:.2f}', loc='upper right')\n",
    "\n",
    "    # Set plot title and labels\n",
    "    plt.title(f'Bland-Altman Plot: {scale} Differences Between Annotator Consensus and Patient-Reported vs the Mean Score')\n",
    "    plt.xlabel('Mean Score')\n",
    "    plt.ylabel('Difference (Mean across Annotators - Self-Reported)')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "for scale in scales:\n",
    "    # Filter DataFrame for the current scale\n",
    "    consensus_df = patient_level_consensus_df[patient_level_consensus_df['Scale'] == scale]\n",
    "    # Set custom color palette based on the scale\n",
    "    if scale == 'Fitzpatrick':\n",
    "        custom_palette = fitzpatrick_palette\n",
    "    elif scale == 'Monk':\n",
    "        custom_palette = monk_palette\n",
    "    else: \n",
    "        custom_palette = \"Set3\"\n",
    "\n",
    "    # Perform the analyses\n",
    "    scatter_spearman_consensus(consensus_df, scale)\n",
    "    boxplot_jitter_consensus(consensus_df, scale, custom_palette)\n",
    "    violin_plot_consensus(consensus_df, scale, custom_palette)\n",
    "    bland_altman_plot_consensus(consensus_df, scale)\n",
    "    bland_altman_plot_consensus_2(consensus_df, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reorganize Master DF \n",
    "\n",
    "def prepare_data_with_all_ratings(master_df, scale, annotators, num_images=9):\n",
    "    grouped_data = {}\n",
    "    \n",
    "    for subject_id in master_df['Subject_ID'].unique():\n",
    "        subject_data = master_df[master_df['Subject_ID'] == subject_id]\n",
    "        \n",
    "        ratings = {'Subject_ID': subject_id}\n",
    "        \n",
    "        # Capture each annotator's scores across all images\n",
    "        for annotator in annotators:\n",
    "            for img_num in range(1, num_images + 1):\n",
    "                score_col = f'{scale}_Score_Annotator{annotator}'\n",
    "                img_col_name = f'{score_col}_Image{img_num}'\n",
    "                \n",
    "                # Extract rating if available\n",
    "                if len(subject_data) >= img_num:\n",
    "                    ratings[img_col_name] = subject_data[score_col].iloc[img_num - 1] if pd.notna(subject_data[score_col].iloc[img_num - 1]) else None\n",
    "                else:\n",
    "                    ratings[img_col_name] = None  # No rating for this image\n",
    "            \n",
    "        # Include the subject score if available\n",
    "        ratings[f'{scale}_Subject_Score'] = subject_data[f'{scale} Score'].iloc[0] if f'{scale} Score' in subject_data.columns else None\n",
    "\n",
    "        grouped_data[subject_id] = ratings\n",
    "\n",
    "    grouped_df = pd.DataFrame.from_dict(grouped_data, orient='index').reset_index(drop=True)\n",
    "    return grouped_df\n",
    "\n",
    "def calculate_mean_scores(grouped_df, scale, annotators, num_images=9):\n",
    "    # Calculate the mean score across all images for each patient and annotator\n",
    "    mean_scores = {}\n",
    "    for annotator in annotators:\n",
    "        score_cols = [f'{scale}_Score_Annotator{annotator}_Image{i+1}' for i in range(num_images)]\n",
    "        mean_scores[f'{scale}_Score_Annotator{annotator}'] = grouped_df[score_cols].mean(axis=1)\n",
    "    \n",
    "    # Combine into a single DataFrame with Subject_ID as the index\n",
    "    mean_scores_df = pd.DataFrame(mean_scores)\n",
    "    mean_scores_df['Subject_ID'] = grouped_df['Subject_ID']\n",
    "    \n",
    "    # Set Subject_ID as the index\n",
    "    mean_scores_df = mean_scores_df.set_index('Subject_ID')\n",
    "    return mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Maker Helper Function\n",
    "\n",
    "def table_maker(data, columns, scale, title):\n",
    "    # Create a Matplotlib figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(18, 5))\n",
    "\n",
    "    # Hide the axes\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create a table\n",
    "    table = ax.table(\n",
    "        cellText=data,\n",
    "        colLabels=columns,\n",
    "        loc=\"center\",\n",
    "        cellLoc=\"center\",\n",
    "    )\n",
    "\n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "\n",
    "    # Make the header row bold and give it a background color\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row == 0:  # Header row\n",
    "            cell.set_fontsize(11)\n",
    "            cell.set_text_props(weight=\"bold\")  # Make text bold\n",
    "            cell.set_facecolor(\"#f4f4f4\")  # Light gray background\n",
    "            cell.set_edgecolor(\"black\")  # Black border\n",
    "        elif row % 2 == 0:  # Alternating row colors\n",
    "            cell.set_facecolor(\"#f9f9f9\")  # Slightly lighter background\n",
    "        else:\n",
    "            cell.set_facecolor(\"white\")  # Standard background\n",
    "\n",
    "    # Add a title above the table\n",
    "    plt.title(\n",
    "        title,\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.subplots_adjust(top=1)\n",
    "\n",
    "    # Save as image or PDF\n",
    "    plt.savefig(f\"styled_icc_table_{scale}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Inter-Annotator Agreement Stats\n",
    "from pingouin import intraclass_corr, friedman\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Agreement Calculations\n",
    "def agreement_measures_expanded(grouped_df, scale, annotators, num_images=9):\n",
    "    # ICC Calculation: Prepare data for ICC by reshaping the DataFrame\n",
    "    icc_data = pd.melt(\n",
    "        grouped_df,\n",
    "        id_vars=['Subject_ID'],\n",
    "        value_vars=[col for col in grouped_df.columns if f'{scale}_Score_Annotator' in col],\n",
    "        var_name='Rater_Image',\n",
    "        value_name='Score'\n",
    "    ).dropna()\n",
    "    \n",
    "    icc_data['Rater'] = icc_data['Rater_Image'].apply(lambda x: x.split('_')[2])  # Extract annotator ID\n",
    "    icc_results = intraclass_corr(data=icc_data, targets='Subject_ID', raters='Rater', ratings='Score', nan_policy='omit').round(3)\n",
    "    print(f\"Intraclass Correlation (ICC) Results for {scale}:\")\n",
    "    print(icc_results)\n",
    "\n",
    "    # Calculate pairwise Weighted Cohen's Kappa for each annotator pair across all images\n",
    "    weighted_cohen_kappa_results = {}\n",
    "    for i, annotator1 in enumerate(annotators):\n",
    "        for annotator2 in annotators[i + 1:]:\n",
    "            kappas = []\n",
    "            for img_num in range(1, num_images + 1):\n",
    "                score1 = grouped_df[f'{scale}_Score_Annotator{annotator1}_Image{img_num}'].dropna()\n",
    "                score2 = grouped_df[f'{scale}_Score_Annotator{annotator2}_Image{img_num}'].dropna()\n",
    "                if not score1.empty and not score2.empty:\n",
    "                    kappa = cohen_kappa_score(score1.round(), score2.round(), weights='quadratic')\n",
    "                    kappas.append(kappa)\n",
    "            if kappas:\n",
    "                avg_kappa = np.mean(kappas)\n",
    "                print(f\"Avg Weighted Cohen’s Kappa ({scale}): Annotator {annotator1} vs Annotator {annotator2} = {avg_kappa:.2f}\")\n",
    "                weighted_cohen_kappa_results[f\"Annotator {annotator1} vs Annotator {annotator2}\"] = [round(avg_kappa, 2)]\n",
    "    print(weighted_cohen_kappa_results)\n",
    "    return icc_results, pd.DataFrame(weighted_cohen_kappa_results, index=[0])\n",
    "\n",
    "def calculate_kendalls_w_and_kripp_alpha(mean_scores_df, scale):\n",
    "    # Print the data used for testing\n",
    "    #print(\"Data for Kendall's W and Krippendorff's Alpha calculation:\\n\", mean_scores_df.head())\n",
    "\n",
    "    # Kendall's W Calculation using friedman (from pingouin)\n",
    "    kendall_w_value = None\n",
    "    if not mean_scores_df.empty:\n",
    "        friedman_result = friedman(mean_scores_df)\n",
    "        kendall_w_value = friedman_result['W'].values[0]  # Extract Kendall's W\n",
    "        print(f\"Kendall's W for {scale}: {kendall_w_value:.2f}\")\n",
    "\n",
    "    # Krippendorff’s Alpha calculation\n",
    "    kripp_alpha_value = krippendorff.alpha(mean_scores_df.to_numpy())\n",
    "    print(f\"Krippendorff's Alpha for {scale}: {kripp_alpha_value:.2f}\")\n",
    "    \n",
    "    return kendall_w_value, kripp_alpha_value\n",
    "\n",
    "# Example usage\n",
    "scales = ['Fitzpatrick', 'Monk']\n",
    "annotators = [1, 2, 3]\n",
    "num_images = 9\n",
    "\n",
    "inter_results = {f\"Kendall's W\": {}, \"Krippendorff's Alpha\": {}}\n",
    "for scale in scales:\n",
    "    grouped_df = prepare_data_with_all_ratings(master_df, scale, annotators, num_images)\n",
    "    icc_results, weighted_ck = agreement_measures_expanded(grouped_df, scale, annotators)\n",
    "    mean_scores_df = calculate_mean_scores(grouped_df, scale, annotators, num_images)\n",
    "    \n",
    "    # Calculate agreement metrics using mean scores\n",
    "    kendallW, kripp_alpha = calculate_kendalls_w_and_kripp_alpha(mean_scores_df, scale)\n",
    "    inter_results[f\"Kendall's W\"][scale] = round(kendallW, 2)\n",
    "    inter_results[f\"Krippendorff's Alpha\"][scale] = round(kripp_alpha, 2) \n",
    "\n",
    "    # make a table for each of the results\n",
    "    data = icc_results.values\n",
    "    columns = icc_results.columns\n",
    "    table_maker(data, columns, scale, f\"Table X. Intraclass Correlation (ICC) Results for {scale}\")\n",
    "\n",
    "    data = weighted_ck.values\n",
    "    columns = weighted_ck.columns\n",
    "    table_maker(data, columns, scale, f\"Table X. Weighted Cohen's Kappa Results for {scale}\")\n",
    "\n",
    "inter_results = pd.DataFrame(inter_results)\n",
    "data = inter_results.values\n",
    "columns = inter_results.columns\n",
    "table_maker(data, columns, None, \"Table X. Kendall's W and Krippendoff Values by scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Linear Mixed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def elongate_data_for_model(master_df, scale):\n",
    "    # Define columns for annotator scores and confidence levels\n",
    "    annotator_score_cols = [f'{scale}_Score_Annotator{i}' for i in range(1, 4)]\n",
    "    annotator_confidence_cols = [f'{scale}_Confidence_Annotator{i}' for i in range(1, 4)]\n",
    "    \n",
    "    # Create a long-format DataFrame for annotator-image combinations\n",
    "    long_df = pd.DataFrame()\n",
    "    \n",
    "    for i, (score_col, conf_col) in enumerate(zip(annotator_score_cols, annotator_confidence_cols), start=1):\n",
    "        annotator_df = master_df[['Image_ID', 'Subject_ID', 'Location', 'Timestamp', f'{scale} Score']].copy()\n",
    "        annotator_df['Annotator'] = i\n",
    "        annotator_df['Rating'] = master_df[score_col]\n",
    "        annotator_df['Confidence'] = master_df[conf_col]\n",
    "        annotator_df['Difference'] = annotator_df['Rating'] - annotator_df[f'{scale} Score']\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        annotator_df = annotator_df.rename(columns={f'{scale} Score': 'SelfReportedScore', 'Location': 'Position', 'Subject_ID': 'PatientID'})\n",
    "        \n",
    "        # Append to long_df\n",
    "        long_df = pd.concat([long_df, annotator_df], ignore_index=True)\n",
    "    \n",
    "    # Convert columns to categorical where appropriate\n",
    "    long_df['Confidence'] = pd.Categorical(long_df['Confidence'], ordered=True)\n",
    "    long_df['Annotator'] = long_df['Annotator'].astype('category')\n",
    "    long_df['PatientID'] = long_df['PatientID'].astype('category')\n",
    "    long_df['Position'] = long_df['Position'].astype('category')\n",
    "    \n",
    "    return long_df\n",
    "\n",
    "def run_mixed_effects_model(master_df, scale):\n",
    "    # Prepare the elongated data for the model\n",
    "    df = elongate_data_for_model(master_df, scale)\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # drop the unique id column\n",
    "    df = df.drop(columns=['Image_ID', 'Timestamp'])\n",
    "    print(df.head())\n",
    "    \n",
    "    # Formula for the mixed-effects model\n",
    "    formula = 'Difference ~ SelfReportedScore + Confidence + Position'\n",
    "    \n",
    "    # Specify the variance component for 'Annotator' as a random effect\n",
    "    vc_formula = {'Annotator': '0 + C(Annotator)'}\n",
    "    \n",
    "    # Run the mixed-effects model using PatientID as the grouping variable (has duplicates??? )\n",
    "    md = smf.mixedlm(formula, df, groups=df['PatientID'], vc_formula=vc_formula)\n",
    "    mixed_model = md.fit()\n",
    "    \n",
    "    # Output results\n",
    "    return mixed_model.summary()\n",
    "\n",
    "# Example usage\n",
    "for scale in ['Fitzpatrick', 'Monk']:\n",
    "    print(f'{scale} Scale:')\n",
    "    summary = run_mixed_effects_model(master_df, scale)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Paired T-Test\n",
    "- The compares the mean annotators conseus as a group versus the patient's subjective score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def perform_t_test(mean_scores_df, scale, master_df):\n",
    "    # get consensus scores\n",
    "    mean_scores_df[f'Mean_Annotator_Score'] = mean_scores_df[[f'{scale}_Score_Annotator1', f'{scale}_Score_Annotator2', f'{scale}_Score_Annotator3']].mean(axis=1)\n",
    "  \n",
    "    # Get the subjective scores from the master_df\n",
    "    subj_scores = master_df[['Subject_ID', f'{scale} Score']].drop_duplicates()\n",
    "    \n",
    "    # ALign by Subject ID\n",
    "    subj_scores = subj_scores.reset_index()\n",
    "    mean_scores_df = mean_scores_df.reset_index()\n",
    "\n",
    "    # Merge the two dataframes\n",
    "    merged_df = pd.merge(subj_scores, mean_scores_df, on='Subject_ID')\n",
    "\n",
    "    # drop any rows with missing values\n",
    "    #merged_df = merged_df.dropna()\n",
    "\n",
    "    # Perform the paired t-test\n",
    "    t_stat, p_val = stats.ttest_rel(merged_df['Mean_Annotator_Score'], merged_df[f'{scale} Score'])\n",
    "        \n",
    "    # Print and store results\n",
    "    print(f'{scale} Scale - T-Statistic: {t_stat:.2f}, P-Value: {p_val:.3f}')\n",
    "    return {'T-Statistic': t_stat, 'P-Value': p_val}\n",
    "\n",
    "\n",
    "annotators = [1, 2, 3]\n",
    "for scale in ['Fitzpatrick', 'Monk']:\n",
    "    grouped_df = prepare_data_with_all_ratings(master_df, scale, annotators, 9)\n",
    "    mean_scores_df = calculate_mean_scores(grouped_df, scale, annotators)\n",
    "\n",
    "    # Perform the t-test\n",
    "    results = perform_t_test(mean_scores_df, scale, master_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkinToneStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
